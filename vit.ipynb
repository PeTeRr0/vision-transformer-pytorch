{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOopwujWHXAPuWg2Ft3vIW3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"8ylkDx-gEGmK","executionInfo":{"status":"ok","timestamp":1753696726091,"user_tz":-540,"elapsed":16,"user":{"displayName":"Eunsol Choi","userId":"06868299113215876249"}}},"outputs":[],"source":["#@title ViT Implementaiton\n","\n","import torch\n","from torch import nn\n","import math\n","\n","class PatchEmbedding(nn.Module):\n","  \"\"\"\n","  Convert the image into patches and then project them into a vector space.\n","  \"\"\"\n","  def __init__(self, config):\n","    super().__init__()\n","    self.config = config\n","    self.img_size = config[\"img_size\"]\n","    self.patch_size = config[\"patch_size\"]\n","    self.channels = config[\"channels\"]\n","    self.hidden_size = config[\"hidden_size\"]\n","    self.num_patches = (self.img_size // self.patch_size) ** 2\n","    # Create a projection layer to convert the image into patches\n","    # The layer projects each patches into a vector (hidden size)\n","    self.projection = nn.Conv2d(\n","        self.num_patches,\n","        self.hidden_size,\n","        kernel_size=self.patch_size,\n","        stride=self.patch_size\n","    )\n","    # Creat a learnable [CLS] token\n","    # The [CLS] token is added to the beginning of the input sequence\n","    self.cls_token = nn.Parameter(torch.zeros(1, 1, config[\"self.hidden_size\"]))\n","    # Create position embeddings for the [CLS] token and the patch embedidngs\n","    # Add 1 to the sequence length for the [CLS] token\n","    self.pos_embedding = nn.Parameter(torch.zeros(1, 1 + self.num_patches, config[\"self.hidden_size\"]))\n","    self.dropout = nn.Dropout(config[\"dropout\"])\n","\n","  def forward(self, x):\n","    b, _, _, _ = x.shape\n","    x = self.projection(x)\n","    x = x.flatten(2)\n","    x = x.transpose(1, 2)\n","\n","    # Expand the [CLS] token to the batch size\n","    # (1, 1, h) -> (b, 1, h)\n","    cls_tokens = self.cls_token.expand(b, -1, -1)\n","    # Concatenate the [CLS] token to the beginning of the input sequence\n","    # This results in a sequence length of (num_patches + 1)\n","    x = torch.cat((cls_tokens, x), dim=1)\n","    x += self.pos_embedding\n","    x = self.dropout(x)\n","    return x\n","\n","class Attention(nn.Module):\n","  \"\"\"\n","  Attention Head for MHA\n","  \"\"\"\n","  def __init__(self, hidden_size, attention_head_size, dropout, bias=False):\n","    super().__init__()\n","    self.hidden_size = hidden_size\n","    self.attention_head_size = attention_head_size\n","\n","    # Create the query, key and value projection layers\n","    self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n","    self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n","    self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n","\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x):\n","    # Project the input into query, key and value\n","    # The same input is used to generate the query, key and value\n","    # So it's called self-attention\n","    query = self.query(x)\n","    key = self.key(x)\n","    value = self.value(x)\n","    # Calculate attention score\n","    # softmax(q * k.T / sqrt(head_size)) * v\n","    attention_scores = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.attention_head_size)\n","    attention_probs = torch.softmax(attention_scores, dim=-1)\n","    attention_probs = self.dropout(attention_probs)\n","    # Calculate the attention output\n","    attention_output = torch.matmul(attention_probs, value)\n","    return (attention_output, attention_probs)\n","\n","class MultiHeadAttention(nn.Module):\n","  \"\"\"\n","  MHA (Multi-Head Attention)\n","  \"\"\"\n","  def __init__(self, config):\n","    super().__init__()\n","    self.config = config\n","    self.hidden_size = config[\"hidden_size\"]\n","    self.num_attention_heads = config[\"num_attention_heads\"]\n","    self.attention_head_size = self.num_attention_heads * self.hidden_size\n","    # Whether or not to use bias in the query, key, and value porjection layers\n","    self.qkv_bias = config[\"qkv_bias\"]\n","    # Create a list of attention heads\n","    self.heads = nn.ModuleList([])\n","    for _ in range(self.num_attention_heads):\n","      self.heads.append(Attention(self.hidden_size, self.attention_head_size, config[\"dropout\"], self.qkv_bias))\n","    # Create a linear layer to project the attention output back to the hidden size\n","    self.output_projection = nn.Linear(self.attention_head_size, self.hidden_size)\n","    self.output_dropout = nn.Dropout(config[\"dropout\"])\n","\n","  def forward(self, x):\n","    # Calculate the attention output for each attention head\n","    attention_outputs = []\n","    for head in self.heads:\n","      attention_out, _ = head(x)\n","      attention_outputs.append(attention_out)\n","    # Concatenate the attention outputs from each attention head\n","    attention_output = []\n","    for attention_output in attention_outputs:\n","      attention_output = torch.cat(attention_outputs, dim=-1)\n","    # Project the concatenated attention output back to the hidden size\n","    attention_output = self.output_projection(attention_output)\n","    attention_output = self.output_dropout(attention_output)\n","    return attention_output\n","\n","class MLP(nn.Module):\n","  \"\"\"\n","  Multi-layer perceptron\n","  Feedforward\n","  \"\"\"\n","  def __init__(self, config):\n","    super().__init__()\n","    self.net = nn.Sequential(\n","        nn.LayerNorm(config[\"hidden_size\"]),\n","        nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"]),\n","        nn.GELU(),\n","        nn.Dropout(config[\"dropout\"]),\n","        nn.Linear(config(\"intermediate_size\"), config[\"hidden_size\"]),\n","        nn.Dropout(config[\"dropout\"])\n","    )\n","\n","  def forward(self, x):\n","    return self.net(x)\n","\n","class TransformerBlock(nn.Module):\n","  \"\"\"\n","  Transformer Block\n","  \"\"\"\n","  def __init__(self, config):\n","    super().__init__()\n","    nn.LayerNorm(config[\"hidden_size\"])\n","    self.mlp = MLP(config)\n","\n","  def forward(self, x):\n","    # Self-attention\n","    attention_output = MultiHeadAttention(x)\n","    x += attention_output\n","    # Feedforward\n","    mlp_output = self.mlp(x)\n","    x += mlp_output\n","    return x\n","\n","class TransformerEncoder(nn.Module):\n","  \"\"\"\n","  Transformer Encoder\n","  \"\"\"\n","  def __init__(self, config):\n","    super().__init__()\n","    # Create a list of transformer blocks\n","    self.blocks = nn.ModuleList([])\n","    for _ in range(config[\"num_hidden_layers\"]):\n","      self.blocks.append(TransformerBlock(config))\n","\n","  def forward(self, x):\n","    # Calculate the transformer block's output for each block\n","    for block in self.blocks:\n","      x = block(x)\n","\n","    return x\n","\n","class ViT(nn.Module):\n","  \"\"\"\n","  ViT model and classification\n","  \"\"\"\n","  def __init__(self, config):\n","    super().__init__()\n","    self.config = config\n","    self.img_size = config[\"img_size\"]\n","    self.hidden_size = config[\"hidden_size\"]\n","    self.num_classes = config[\"num_classes\"]\n","    # Create the embedding module\n","    self.embedding = PatchEmbedding(config)\n","    # Create the transformer encoder module\n","    self.encoder = TransformerEncoder(config)\n","    # Create a linear layer to project the encoder's output to the number of classes\n","    self.classifier = nn.Linear(config[\"hidden_size\"], config[\"num_classes\"])\n","    # Initialize the weights\n","    self.__init__weights()\n","\n","  def forward(self, x):\n","    # Calculate the embedding output\n","    embedding_output = self.embedding(x)\n","    # Calculate the encoder's output\n","    encoder_output = self.encoder(embedding_output)\n","    # Calculate the logits take the [CLS] token's output as features for classification\n","    logits = self.classifier(encoder_output[:, 0])\n","\n","    return logits\n","\n","  def _init_weights(self, module):\n","    if isinstance(module, (nn.Linear, nn.Conv2d)):\n","      module.weight.data.normal_(mean=0.0, std=0.02)\n","      if module.bias is not None:\n","        module.bias.data.zero_()"]},{"cell_type":"code","source":["#@title data preparation\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","def data_preparation(batch_size=4, num_workers=2, train_sample_size=None, test_sample_size=None):\n","  train_transform = transforms.Compose([\n","      transforms.ToTensor(),\n","      transforms.Resize((32, 32)),\n","      transforms.RandomHorizontalFlip(p=0.5),\n","      transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.75, 1.33333333333333), interpolation=2),\n","      transforms.Normalize((0.5 ,0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","  train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n","\n","  train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","\n","  test_transform = transforms.Compose([\n","      transforms.ToTensor(),\n","      transforms.Resize((32, 32)),\n","      transforms.Normalize((0.5 ,0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","  test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=train_transform)\n","\n","  test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","  classes = ('plane', 'car', 'bird', 'cat',\n","            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","  return train_loader, test_loader, classes"],"metadata":{"id":"_YjXxKqMeC9G","executionInfo":{"status":"ok","timestamp":1753696726156,"user_tz":-540,"elapsed":39,"user":{"displayName":"Eunsol Choi","userId":"06868299113215876249"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#@title Utils\n","\n","import json, os, math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch.nn import functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","\n","def save_experiment(experiment_name, config, model, train_losses, test_losses, accuracies, base_dir=\"experiments\"):\n","    outdir = os.path.join(base_dir, experiment_name)\n","    os.makedirs(outdir, exist_ok=True)\n","\n","    # Save the config\n","    configfile = os.path.join(outdir, 'config.json')\n","    with open(configfile, 'w') as f:\n","        json.dump(config, f, sort_keys=True, indent=4)\n","\n","    # Save the metrics\n","    jsonfile = os.path.join(outdir, 'metrics.json')\n","    with open(jsonfile, 'w') as f:\n","        data = {\n","            'train_losses': train_losses,\n","            'test_losses': test_losses,\n","            'accuracies': accuracies,\n","        }\n","        json.dump(data, f, sort_keys=True, indent=4)\n","\n","    # Save the model\n","    save_checkpoint(experiment_name, model, \"final\", base_dir=base_dir)\n","\n","def save_checkpoint(experiment_name, model, epoch, base_dir=\"experiments\"):\n","    outdir = os.path.join(base_dir, experiment_name)\n","    os.makedirs(outdir, exist_ok=True)\n","    cpfile = os.path.join(outdir, f'model_{epoch}.pt')\n","    torch.save(model.state_dict(), cpfile)\n","\n","\n","def load_experiment(experiment_name, checkpoint_name=\"model_final.pt\", base_dir=\"experiments\"):\n","    outdir = os.path.join(base_dir, experiment_name)\n","    # Load the config\n","    configfile = os.path.join(outdir, 'config.json')\n","    with open(configfile, 'r') as f:\n","        config = json.load(f)\n","    # Load the metrics\n","    jsonfile = os.path.join(outdir, 'metrics.json')\n","    with open(jsonfile, 'r') as f:\n","        data = json.load(f)\n","    train_losses = data['train_losses']\n","    test_losses = data['test_losses']\n","    accuracies = data['accuracies']\n","    # Load the model\n","    model = ViT(config)\n","    cpfile = os.path.join(outdir, checkpoint_name)\n","    model.load_state_dict(torch.load(cpfile))\n","    return config, model, train_losses, test_losses, accuracies\n","\n","\n","def visualize_images():\n","    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                            download=True)\n","    classes = ('plane', 'car', 'bird', 'cat',\n","            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","    # Pick 30 samples randomly\n","    indices = torch.randperm(len(trainset))[:30]\n","    images = [np.asarray(trainset[i][0]) for i in indices]\n","    labels = [trainset[i][1] for i in indices]\n","    # Visualize the images using matplotlib\n","    fig = plt.figure(figsize=(10, 10))\n","    for i in range(30):\n","        ax = fig.add_subplot(6, 5, i+1, xticks=[], yticks=[])\n","        ax.imshow(images[i])\n","        ax.set_title(classes[labels[i]])\n","\n","@torch.no_grad()\n","def visualize_attention(model, output=None, device=\"cuda\"):\n","    \"\"\"\n","    Visualize the attention maps of the first 4 images.\n","    \"\"\"\n","    model.eval()\n","    # Load random images\n","    num_images = 30\n","    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n","    classes = ('plane', 'car', 'bird', 'cat',\n","            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","    # Pick 30 samples randomly\n","    indices = torch.randperm(len(testset))[:num_images]\n","    raw_images = [np.asarray(testset[i][0]) for i in indices]\n","    labels = [testset[i][1] for i in indices]\n","    # Convert the images to tensors\n","    test_transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","        transforms.Resize((32, 32)),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","    images = torch.stack([test_transform(image) for image in raw_images])\n","    # Move the images to the device\n","    images = images.to(device)\n","    model = model.to(device)\n","    # Get the attention maps from the last block\n","    logits, attention_maps = model(images, output_attentions=True)\n","    # Get the predictions\n","    predictions = torch.argmax(logits, dim=1)\n","    # Concatenate the attention maps from all blocks\n","    attention_maps = torch.cat(attention_maps, dim=1)\n","    # select only the attention maps of the CLS token\n","    attention_maps = attention_maps[:, :, 0, 1:]\n","    # Then average the attention maps of the CLS token over all the heads\n","    attention_maps = attention_maps.mean(dim=1)\n","    # Reshape the attention maps to a square\n","    num_patches = attention_maps.size(-1)\n","    size = int(math.sqrt(num_patches))\n","    attention_maps = attention_maps.view(-1, size, size)\n","    # Resize the map to the size of the image\n","    attention_maps = attention_maps.unsqueeze(1)\n","    attention_maps = F.interpolate(attention_maps, size=(32, 32), mode='bilinear', align_corners=False)\n","    attention_maps = attention_maps.squeeze(1)\n","    # Plot the images and the attention maps\n","    fig = plt.figure(figsize=(20, 10))\n","    mask = np.concatenate([np.ones((32, 32)), np.zeros((32, 32))], axis=1)\n","    for i in range(num_images):\n","        ax = fig.add_subplot(6, 5, i+1, xticks=[], yticks=[])\n","        img = np.concatenate((raw_images[i], raw_images[i]), axis=1)\n","        ax.imshow(img)\n","        # Mask out the attention map of the left image\n","        extended_attention_map = np.concatenate((np.zeros((32, 32)), attention_maps[i].cpu()), axis=1)\n","        extended_attention_map = np.ma.masked_where(mask==1, extended_attention_map)\n","        ax.imshow(extended_attention_map, alpha=0.5, cmap='jet')\n","        # Show the ground truth and the prediction\n","        gt = classes[labels[i]]\n","        pred = classes[predictions[i]]\n","        ax.set_title(f\"gt: {gt} / pred: {pred}\", color=(\"green\" if gt==pred else \"red\"))\n","    if output is not None:\n","        plt.savefig(output)\n","    plt.show()"],"metadata":{"id":"l5kgqw-Fjw0P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Train ViT\n","\n","exp_name = 'vit-with-10-epochs' #@param {type:\"string\"}\n","batch_size = 32 #@param {type: \"integer\"}\n","epochs = 10 #@param {type: \"integer\"}\n","lr = 1e-2  #@param {type: \"number\"}\n","save_model_every = 0 #@param {type: \"integer\"}\n","\n","import torch\n","from torch import nn, optim\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","config = {\n","    \"patch_size\": 4,  # Input image size: 32x32 -> 8x8 patches\n","    \"hidden_size\": 48,\n","    \"num_hidden_layers\": 4,\n","    \"num_attention_heads\": 4,\n","    \"intermediate_size\": 4 * 48, # 4 * hidden_size\n","    \"hidden_dropout_prob\": 0.0,\n","    \"attention_probs_dropout_prob\": 0.0,\n","    \"initializer_range\": 0.02,\n","    \"image_size\": 32,\n","    \"num_classes\": 10, # num_classes of CIFAR10\n","    \"num_channels\": 3,\n","    \"qkv_bias\": True,\n","    \"use_faster_attention\": True,\n","}\n","# These are not hard constraints, but are used to prevent misconfigurations\n","assert config[\"hidden_size\"] % config[\"num_attention_heads\"] == 0\n","assert config['intermediate_size'] == 4 * config['hidden_size']\n","assert config['image_size'] % config['patch_size'] == 0\n","\n","class Trainer:\n","    \"\"\"\n","    The simple trainer.\n","    \"\"\"\n","\n","    def __init__(self, model, optimizer, loss_fn, exp_name, device):\n","        self.model = model.to(device)\n","        self.optimizer = optimizer\n","        self.loss_fn = loss_fn\n","        self.exp_name = exp_name\n","        self.device = device\n","\n","    def train(self, trainloader, testloader, epochs, save_model_every_n_epochs=0):\n","        \"\"\"\n","        Train the model for the specified number of epochs.\n","        \"\"\"\n","        # Keep track of the losses and accuracies\n","        train_losses, test_losses, accuracies = [], [], []\n","        # Train the model\n","        for i in range(epochs):\n","            train_loss = self.train_epoch(trainloader)\n","            accuracy, test_loss = self.evaluate(testloader)\n","            train_losses.append(train_loss)\n","            test_losses.append(test_loss)\n","            accuracies.append(accuracy)\n","            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n","            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != epochs:\n","                print('\\tSave checkpoint at epoch', i+1)\n","                save_checkpoint(self.exp_name, self.model, i+1)\n","        # Save the experiment\n","        save_experiment(self.exp_name, config, self.model, train_losses, test_losses, accuracies)\n","\n","    def train_epoch(self, trainloader):\n","        \"\"\"\n","        Train the model for one epoch.\n","        \"\"\"\n","        self.model.train()\n","        total_loss = 0\n","        for batch in trainloader:\n","            # Move the batch to the device\n","            batch = [t.to(self.device) for t in batch]\n","            images, labels = batch\n","            # Zero the gradients\n","            self.optimizer.zero_grad()\n","            # Calculate the loss\n","            loss = self.loss_fn(self.model(images)[0], labels)\n","            # Backpropagate the loss\n","            loss.backward()\n","            # Update the model's parameters\n","            self.optimizer.step()\n","            total_loss += loss.item() * len(images)\n","        return total_loss / len(trainloader.dataset)\n","\n","    @torch.no_grad()\n","    def evaluate(self, testloader):\n","        self.model.eval()\n","        total_loss = 0\n","        correct = 0\n","        with torch.no_grad():\n","            for batch in testloader:\n","                # Move the batch to the device\n","                batch = [t.to(self.device) for t in batch]\n","                images, labels = batch\n","\n","                # Get predictions\n","                logits, _ = self.model(images)\n","\n","                # Calculate the loss\n","                loss = self.loss_fn(logits, labels)\n","                total_loss += loss.item() * len(images)\n","\n","                # Calculate the accuracy\n","                predictions = torch.argmax(logits, dim=1)\n","                correct += torch.sum(predictions == labels).item()\n","        accuracy = correct / len(testloader.dataset)\n","        avg_loss = total_loss / len(testloader.dataset)\n","        return accuracy, avg_loss\n","\n","\n","def main():\n","    # Training parameters\n","    save_model_every_n_epochs = save_model_every\n","    # Load the CIFAR10 dataset\n","    trainloader, testloader, _ = data_preparation(batch_size=batch_size)\n","    # Create the model, optimizer, loss function and trainer\n","    model = ViT(config)\n","    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n","    loss_fn = nn.CrossEntropyLoss()\n","    trainer = Trainer(model, optimizer, loss_fn, exp_name, device=device)\n","    trainer.train(trainloader, testloader, epochs, save_model_every_n_epochs=save_model_every_n_epochs)\n","\n","\n","if __name__ == '__main__':\n","    main()\n",""],"metadata":{"id":"fRpELQnmkEMD"},"execution_count":null,"outputs":[]}]}